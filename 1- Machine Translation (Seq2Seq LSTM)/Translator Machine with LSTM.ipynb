{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6477421",
   "metadata": {},
   "source": [
    "# Machine Translation with LSTM\n",
    "\n",
    "Machine translation is one interesting application in the domain of Natural Language Processing (NLP). Here, we implement a translator machine (German to English) using LSTMs. \n",
    "\n",
    "Most ideas in this notebook are inspired by [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c9962ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Torchtext==0.9 in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: torch==1.8.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from Torchtext==0.9) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from Torchtext==0.9) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from Torchtext==0.9) (1.21.5)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from Torchtext==0.9) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from torch==1.8.0->Torchtext==0.9) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->Torchtext==0.9) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->Torchtext==0.9) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->Torchtext==0.9) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests->Torchtext==0.9) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tqdm->Torchtext==0.9) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orchtext (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchtext (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchtext (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchtext (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchtext (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchtext (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\programdata\\anaconda3\\envs\\pytorch_env\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#torchtext.legacy is not supported on the defult version\n",
    "!pip install Torchtext==0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa5806f",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Let's first import some useful packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28105cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from torchtext.data.metrics import bleu_score\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71de6c41",
   "metadata": {},
   "source": [
    "### Load and Tokenize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb7fe8",
   "metadata": {},
   "source": [
    "We are now ready to download and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83fc6317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"de_core_news_sm\")\n",
    "\n",
    "\n",
    "spacy_ger = spacy.load('de_core_news_sm')\n",
    "spacy_eng = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddf24e",
   "metadata": {},
   "source": [
    "Let's now build a tokenizer whose main role is to take a sentence as input, and output a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b954da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae8451",
   "metadata": {},
   "source": [
    "The `Field` method in `torchtext` handles the preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36409af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "german = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "english = Field(\n",
    "    tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb7756",
   "metadata": {},
   "source": [
    "It's now time to split the dataset into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "578818f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"), fields=(german, english)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ac673",
   "metadata": {},
   "source": [
    "Using the `build_vocab` method, we create the vocabulary of the input source and output target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9afd50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c6a20",
   "metadata": {},
   "source": [
    "Lastly, we need to pad the inputs to be of the same size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7bc5bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595435b",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Let's write some helper functions that we use later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2799bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "    # print(sentence)\n",
    "\n",
    "    # sys.exit()\n",
    "\n",
    "    # Load german tokenizer\n",
    "    spacy_ger = spacy.load('de_core_news_sm')\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # print(tokens)\n",
    "\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, german.init_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(sentence_tensor)\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e134e",
   "metadata": {},
   "source": [
    "Write a function to compute bleu score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be184576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(data, model, german, english, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in data:\n",
    "        src = vars(example)[\"src\"]\n",
    "        trg = vars(example)[\"trg\"]\n",
    "\n",
    "        prediction = translate_sentence(model, src, german, english, device)\n",
    "        prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4f93f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac39199",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "\n",
    "Our sequence to sequence translator model is composed of two main blocks: Encoder, and decoder.\n",
    "We implement both using LSTM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af0ada93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e5cfa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f2354",
   "metadata": {},
   "source": [
    "Putting everything together, we build our Seq2Seq model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "29695024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6b7f3",
   "metadata": {},
   "source": [
    "### Training Steps\n",
    "\n",
    "We're ready to define everything we need for training our Seq2Seq model. First, we need to set the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "47534d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f5726",
   "metadata": {},
   "source": [
    "Next, we instantiate our encoder, decoder and seq2seq models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e64da254",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)\n",
    "\n",
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb1299",
   "metadata": {},
   "source": [
    "Now that everything is ready, we write the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "466b1020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['teeth', 'leashed', 'leashed', 'index', 'leashed', 'index', 'index', 'sunday', 'spotted', 'spotted', 'spotted', 'meeting', 'meeting', 'web', 'communicating', 'communicating', 'haircut', 'haircut', 'haircut', 'promoting', 'beret', 'roses', 'into', 'location', 'studying', 'entertainers', 'sock', 'nicely', 'wet', 'penske', 'nicely', 'penske', 'sprinkles', 'sprinkles', 'sprinkles', 'lying', 'spotted', 'spotted', 'meeting', 'meeting', 'web', 'tattooed', 'bonding', 'cut', 'into', 'into', 'subway', 'center', 'mane', 'mane']\n",
      "[Epoch 1 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'child', 'in', 'a', 'blue', 'shirt', 'and', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "[Epoch 2 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'with', 'a', '<unk>', '<unk>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '<eos>']\n",
      "[Epoch 3 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'player', 'in', 'a', 'is', 'is', 'a', 'a', '<unk>', 'to', 'a', 'a', '.', '<eos>']\n",
      "[Epoch 4 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'with', 'a', '<unk>', 'is', 'being', 'to', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "[Epoch 5 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'police', 'player', 'with', 'a', 'is', 'being', 'pulled', 'by', 'a', '<unk>', 'of', 'a', '.', '<eos>']\n",
      "[Epoch 6 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'car', 'with', 'a', 'number', 'is', 'being', 'pulled', 'by', 'a', 'large', 'of', 'a', '.', '<eos>']\n",
      "[Epoch 7 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'by', 'a', 'a', 'from', 'a', 'large', '.', '<eos>']\n",
      "[Epoch 8 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'a', 'number', 'pulled', 'by', 'a', 'a', 'by', 'a', 'large', 'bull', '.', '<eos>']\n",
      "[Epoch 9 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'a', 'is', 'pulled', 'by', 'a', 'large', 'of', 'by', 'a', 'large', '.', '.', '<eos>']\n",
      "[Epoch 10 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'with', 'pulled', 'pulled', 'pulled', 'by', 'a', 'large', 'bull', 'by', 'a', '<unk>', '.', '<eos>']\n",
      "[Epoch 11 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'wings', 'pulled', 'pulled', 'by', 'a', 'large', 'bull', 'by', 'a', 'large', '.', '<eos>']\n",
      "[Epoch 12 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'white', '<unk>', 'pulled', 'by', 'a', 'large', '<unk>', 'by', 'a', 'large', '.', '<eos>']\n",
      "[Epoch 13 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'cable', 'by', 'a', 'large', '.', '.', '<eos>']\n",
      "[Epoch 14 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'two', 'men', 'pulled', 'pulled', 'pulled', 'from', 'a', 'large', 'of', 'a', '.', '<eos>']\n",
      "[Epoch 15 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'a', 'damaged', 'is', 'pulled', 'by', 'a', 'boat', 'and', 'a', 'large', '<unk>', '.', '<eos>']\n",
      "[Epoch 16 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'pulled', 'by', 'a', 'large', 'cable', 'and', 'a', 'large', '.', '<eos>']\n",
      "[Epoch 17 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'is', 'pulled', 'by', 'a', 'large', ',', 'and', 'a', 'by', 'a', 'large', 'bull', '.', '<eos>']\n",
      "[Epoch 18 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'many', 'men', 'is', 'pulled', 'by', 'a', 'large', 'bull', 'by', 'a', 'large', 'of', 'horses', '.', '<eos>']\n",
      "[Epoch 19 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'being', 'pulled', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', 'cable', '.', '<eos>']\n",
      "Bleu score 18.61\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, german, english, device, max_length=50\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target)\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plot to tensorboard\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "\n",
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a765e",
   "metadata": {},
   "source": [
    "### Performance Test\n",
    "\n",
    "Let's see how the trained model works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c156f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct English translation: \n",
      " A man with a blue shirt is running in the street\n",
      "Translation by our machine: \n",
      " ['a', 'man', 'in', 'a', 'blue', 'shirt', 'walks', 'on', 'the', 'street', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "eng_sentence = 'A man with a blue shirt is running in the street'\n",
    "sentence = 'Ein Mann mit blauem Hemd läuft auf der Straße.'\n",
    "model.eval()\n",
    "\n",
    "translated_sentence = translate_sentence(\n",
    "    model, sentence, german, english, device, max_length=50)\n",
    "\n",
    "print(f'Correct English translation: \\n {eng_sentence}')\n",
    "print(f\"Translation by our machine: \\n {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c67e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
